{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date of Admission</th>\n",
       "      <th>Urgent</th>\n",
       "      <th>('Urgent', 'Female')</th>\n",
       "      <th>('Urgent', 'Male')</th>\n",
       "      <th>Urgent_Lag_1</th>\n",
       "      <th>Urgent_Lag_2</th>\n",
       "      <th>Urgent_Lag_3</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>dayofyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-11</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-12</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-14</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-15</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date of Admission  Urgent  ('Urgent', 'Female')  ('Urgent', 'Male')  \\\n",
       "0        2019-05-11       9                     6                   3   \n",
       "1        2019-05-12      12                     7                   5   \n",
       "2        2019-05-13       8                     5                   3   \n",
       "3        2019-05-14       8                     3                   5   \n",
       "4        2019-05-15      12                     6                   6   \n",
       "\n",
       "   Urgent_Lag_1  Urgent_Lag_2  Urgent_Lag_3  month  day  year  quarter  \\\n",
       "0             9            15             2      5   11  2019        2   \n",
       "1             9             9            15      5   12  2019        2   \n",
       "2            12             9             9      5   13  2019        2   \n",
       "3             8            12             9      5   14  2019        2   \n",
       "4             8             8            12      5   15  2019        2   \n",
       "\n",
       "   dayofweek  dayofyear  \n",
       "0          5        131  \n",
       "1          6        132  \n",
       "2          0        133  \n",
       "3          1        134  \n",
       "4          2        135  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"C:\\\\Users\\\\Republic Of Gamers\\\\OneDrive\\\\Documents\\\\GitHub\\\\TSDN-BoyWithLuv\\\\Source\\\\Data\\\\sdm_ts_urgent_daily.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Date of Admission', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainUrgentSize = int(len(df) * 0.7)\n",
    "trainUrgent, testUrgent= df[:trainUrgentSize], df[trainUrgentSize:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainUrgent.drop(columns=['Urgent'])\n",
    "y_train = trainUrgent['Urgent']\n",
    "X_test = testUrgent.drop(columns=['Urgent'])\n",
    "y_test = testUrgent['Urgent']\n",
    "X_train.columns = X_train.columns.astype(str)\n",
    "X_test.columns = X_test.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Best parameters found:  {'max_depth': 30, 'min_samples_split': 2, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.         12.          7.99666667  7.98666667  6.         12.99\n",
      "  6.67333333 10.          7.          9.99333333  8.         11.\n",
      " 11.97333333 13.          1.87333333  7.98666667  7.          8.\n",
      " 12.02333333  6.         16.46666667  8.         10.         16.42333333\n",
      " 11.02333333 10.99666667 12.96        6.68666667  9.         15.06666667\n",
      " 10.          9.99666667  9.         13.26666667 12.         12.\n",
      "  7.          8.03666667  6.         13.         13.12666667 10.\n",
      " 15.81333333  5.89333333 12.          7.         15.03666667 12.96333333\n",
      " 15.97666667 10.          9.          7.         11.          6.99666667\n",
      " 14.98       14.         13.         10.          1.69333333  9.86\n",
      " 14.89        8.          6.         12.99666667  4.99        7.99333333\n",
      "  9.          8.         14.00333333  7.         10.99666667  9.09\n",
      " 11.99        7.96666667  5.01        8.         10.          8.\n",
      " 10.          9.         15.04333333  9.          5.          3.50333333\n",
      " 15.64666667 11.         14.53333333  7.          7.         18.92333333\n",
      " 12.          4.99333333  3.22333333 12.00333333 13.47666667  9.00666667\n",
      " 11.         15.03       14.00666667  7.         14.08        8.99\n",
      "  3.23666667 14.00333333  9.06333333 15.01       17.35666667 13.12333333\n",
      " 13.         12.99666667  6.99666667 10.          9.         15.03\n",
      " 10.         12.         16.37333333 11.01333333  6.          6.\n",
      "  7.99       14.76        2.84       12.99       11.          5.02\n",
      "  5.02333333  9.         14.05333333 10.         11.          8.\n",
      "  3.71333333 10.          5.         13.99666667  7.         10.\n",
      " 13.          9.         10.          8.          6.98        3.04666667\n",
      " 13.         11.01666667 14.66666667 11.         15.02333333  7.\n",
      "  2.97333333  7.         11.         11.32       14.06333333  6.87333333\n",
      "  2.98333333  8.68333333  8.          9.          8.          9.\n",
      " 10.         13.         15.89       12.          8.98666667 12.99333333\n",
      "  9.          9.          8.          8.         12.         13.\n",
      " 10.75333333 10.         12.         11.          8.03333333  4.99666667\n",
      "  7.          9.          9.         11.          4.90666667  5.01333333\n",
      " 12.00666667 16.73666667 11.         15.74666667  7.          9.\n",
      "  9.         11.          9.         13.          9.         12.01333333\n",
      " 11.          9.          9.          5.          9.         13.96666667\n",
      "  5.64        9.93666667 11.         11.         11.         13.10666667\n",
      " 12.         12.99666667 10.         14.06333333 12.          9.\n",
      " 11.         11.         12.          6.99       14.00666667 11.\n",
      " 15.04       10.         12.99666667 14.         16.9         9.99333333\n",
      " 16.87333333  4.97666667 11.         12.         11.95666667 17.27\n",
      "  6.          6.         11.         11.          8.         12.\n",
      " 10.14666667 10.19666667 14.60666667 12.          6.         10.\n",
      "  7.          9.          9.26333333  8.          8.         15.03\n",
      "  8.          9.99333333  4.98666667 14.08        9.91       14.\n",
      "  7.         10.         17.14        6.99666667 11.          1.49666667\n",
      " 14.28666667  5.10666667 10.         13.89        7.          9.\n",
      " 10.          4.95666667 11.         12.99666667 10.         14.02333333\n",
      " 18.38333333 13.         15.04       16.88       11.          6.\n",
      " 11.          9.         10.         15.96666667 11.         11.37666667\n",
      " 12.          9.         15.97666667  6.74666667  9.         12.97333333\n",
      "  7.          8.         11.02333333  8.         14.03       14.59\n",
      "  7.          7.04666667  3.96333333 11.          8.         15.05666667\n",
      " 15.96666667  9.         12.99666667  8.          8.          7.\n",
      " 10.         15.96        9.01666667  9.         11.         11.02333333\n",
      " 10.         12.         10.         14.          5.78333333  9.\n",
      "  8.         11.         11.         13.         10.          7.\n",
      " 10.          8.93333333  6.         10.         12.00333333 10.\n",
      "  8.          9.87       13.01333333  8.97        6.          9.91666667\n",
      "  9.          9.00333333 13.          7.63666667  5.00666667 11.\n",
      " 16.29333333  7.          9.         13.         16.01333333  9.\n",
      " 12.         10.         11.         10.         15.98333333  7.\n",
      "  9.         12.01333333 16.44        9.03        7.27666667  6.\n",
      "  8.99666667 15.03        9.02        3.48666667 15.07        9.03666667\n",
      " 14.53666667  7.08333333 11.02333333 15.06       15.34       12.97666667\n",
      "  8.98666667  7.          1.70666667 15.97333333 18.37333333 12.\n",
      " 11.         14.06333333  1.94333333  7.          7.          9.\n",
      " 10.          7.         12.          1.71333333 11.02666667 12.01666667\n",
      " 13.06333333  4.99666667  6.52        9.         15.02666667 11.\n",
      "  6.         17.95333333  7.         10.          8.         10.\n",
      "  9.04333333 11.          7.          6.         10.          9.\n",
      " 12.02333333 10.         11.          9.         10.          9.99\n",
      "  9.          5.75       15.02666667  4.99666667  6.          9.\n",
      "  3.95       12.         12.97666667 13.00333333 10.         13.\n",
      " 12.00666667  4.99333333 12.85        8.         14.00333333 10.\n",
      "  6.         11.         10.77666667 13.98666667 10.         12.98\n",
      " 16.13        6.          6.         18.40666667  7.96666667 12.95666667\n",
      "  9.         15.02333333  8.         11.         14.          8.\n",
      "  9.          8.         12.          8.00666667  7.          8.\n",
      "  6.76333333  6.          7.          7.          8.          3.96\n",
      " 12.          5.00666667  9.76666667  7.96666667  9.          6.\n",
      " 10.          6.         12.95       12.04333333  8.          5.00666667\n",
      " 13.         10.          7.          8.01333333 12.99333333  7.\n",
      "  6.         14.31666667 17.14333333 12.         11.00666667 16.03333333\n",
      " 10.         15.01333333 12.         19.13        8.98333333  8.\n",
      " 10.          7.          3.58666667 15.49        6.         17.16\n",
      "  6.          4.99333333  9.         11.         12.99333333  6.\n",
      "  7.23333333 10.          8.         11.          9.          7.\n",
      " 14.86666667 10.         14.02666667  7.          6.          5.\n",
      " 13.          8.         12.01666667  7.96666667  8.         12.00333333\n",
      "  6.          9.99333333 16.23333333  9.          9.          9.\n",
      " 14.49333333 13.          8.00333333 15.42666667  7.          9.35333333\n",
      " 13.         10.74333333 11.          7.          9.01       12.\n",
      " 12.01       10.        ]\n"
     ]
    }
   ],
   "source": [
    "# Extract the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Initialize the RandomForestRegressor with the best parameters\n",
    "best_rf = RandomForestRegressor(**best_params)\n",
    "\n",
    "# Fit the model to the full training data\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set or future data\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Display predictions\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 0.2912379887633921\n",
      "Mean Absolute Error (MAE): 0.09465328467153286\n",
      "R-squared (R²): 0.9928658560044674\n"
     ]
    }
   ],
   "source": [
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Display the metrics\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-squared (R²):\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Best parameters found:  {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Republic Of Gamers\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\core.py:160: UserWarning: [00:37:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"min_samples_split\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "xgb_model = XGBRegressor()\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Republic Of Gamers\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\core.py:160: UserWarning: [00:37:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"min_samples_split\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [ 8.001598   12.0230055   8.002477    8.02176     5.9789233  12.980762\n",
      "  6.9679356   9.987465    6.966935    9.952893    8.000276   10.998242\n",
      " 11.990795   12.945383    1.8673964   8.01638     6.980525    7.9486\n",
      " 11.970782    5.987874   17.472473    7.9540873   9.974253   16.227406\n",
      " 11.026211   10.946767   12.956558    6.985918    9.011996   14.964556\n",
      "  9.9769945   9.966504    9.001406   13.34433    11.976382   11.968582\n",
      "  6.9908895   7.9458175   6.027978   12.992252   12.666922    9.9808\n",
      " 15.9016      6.0008206  11.975997    6.991905   14.89358    12.95516\n",
      " 15.901685    9.969878    9.004966    6.9931636  11.010959    6.9914804\n",
      " 15.007497   14.090322   13.005444   10.02631     2.085723   10.018078\n",
      " 14.970526    8.027578    6.038591   13.036308    4.981764    8.068464\n",
      "  8.995652    8.009533   14.0246315   6.9765124  10.959566    9.014357\n",
      " 12.000068    8.015311    5.008975    8.00067     9.983415    8.003458\n",
      "  9.984698    9.00282    14.911491    9.010171    5.0002494   2.9774308\n",
      " 15.068062   10.979023   15.428065    6.998211    6.977815   19.410152\n",
      " 11.990375    5.0126123   3.1025927  12.007406   13.325992    9.043411\n",
      " 11.007955   14.873225   14.012428    6.9709563  14.064473    9.043577\n",
      "  2.6458998  14.032884    8.991366   14.914302   16.73501    12.949911\n",
      " 12.949899   12.981201    6.9823294   9.985371    9.016775   14.916324\n",
      "  9.984199   11.975918   16.978056   11.028182    6.0201945   6.040931\n",
      "  7.9316077  14.97166     2.3295972  13.016332   10.970645    5.0140486\n",
      "  5.0177736   9.032588   14.132849    9.982719   11.018208    7.9912825\n",
      "  3.9422405   9.984509    5.016516   13.968679    6.9778557   9.989923\n",
      " 12.993167    9.008146    9.984847    8.01014     7.0008945   3.0995588\n",
      " 12.98774    11.005248   15.435647   11.001362   14.921544    6.986997\n",
      "  3.0327716   6.973107   11.007515   11.077022   14.073634    6.001463\n",
      "  3.0216713   8.434473    8.006752    9.001582    8.007968    9.004905\n",
      "  9.972309   12.943018   15.9284     11.996595    8.547326   12.9702425\n",
      "  9.000396    9.000918    7.9953427   7.997124   12.0018835  12.97872\n",
      " 10.998569    9.9776325  11.996638   11.012075    7.934841    4.163793\n",
      "  6.994271    9.01482     9.008635   10.997342    4.999177    5.0042815\n",
      " 11.997286   17.0279     10.981721   14.950075    6.9964046   9.003968\n",
      "  9.002244   10.992858    8.992047   12.984495    9.00176    12.008782\n",
      " 10.997801    8.997756    9.015259    4.9668827   9.002074   13.971386\n",
      "  5.971441   10.020952   10.997676   11.021175   11.012834   13.236345\n",
      " 11.980741   13.016952    9.984712   14.09032    11.9873295   9.010095\n",
      " 11.005585   11.01688    11.968081    6.983023   14.044926   10.922479\n",
      " 14.964229    9.992348   12.964569   14.056137   17.021782    9.9551\n",
      " 16.980064    4.9826846  10.999064   11.996365   12.031671   16.73683\n",
      "  6.0188413   6.0207396  10.99986    11.001066    8.017604   12.009948\n",
      "  9.906433    9.9304     15.456902   12.005214    6.020285    9.98425\n",
      "  6.97305     9.008323    9.011947    8.001069    8.02013    14.901262\n",
      "  8.003943    9.959237    4.966196   14.087029    9.960376   14.0487795\n",
      "  6.9701447   9.970349   18.419502    6.974746   10.992931    1.0072507\n",
      " 14.510269    5.1128316   9.994027   14.000817    6.9824986   9.016266\n",
      "  9.9791155   4.9873075  11.006442   13.008142    9.997773   13.97169\n",
      " 18.17136    12.968232   14.9610405  17.583786   10.996898    6.0171304\n",
      " 10.991907    9.01221     9.974479   15.836654   11.027806   11.393448\n",
      " 11.978398    9.009057   15.954573    6.966153    8.9999895  12.947246\n",
      "  6.960973    8.022442   11.002834    7.9958625  13.9819975  15.912437\n",
      "  6.9763665   6.982629    3.9462583  11.001943    8.003692   14.878087\n",
      " 16.062176    8.99733    12.961442    8.000049    8.01022     6.9753947\n",
      "  9.976441   15.907122    9.008085    9.011406   10.986724   11.00067\n",
      "  9.986591   12.017192    9.977463   14.005899    5.9814157   9.019327\n",
      "  8.014913   11.009109   10.991502   12.967036    9.984721    6.9837313\n",
      "  9.984462    8.505369    6.031433    9.992178   11.9899435   9.994732\n",
      "  8.003221    9.9789     12.98731     9.050954    6.031969    9.959254\n",
      "  9.026815    9.011045   12.987402    7.043948    4.9941225  10.992278\n",
      " 15.9684305   6.9958844   9.010506   12.967405   16.084785    9.032116\n",
      " 11.99747     9.984563   11.026198    9.976597   16.123178    6.990947\n",
      "  9.003838   11.998996   15.998434    9.065112    7.5073843   5.997859\n",
      "  9.032742   14.922997    8.984825    2.977928   16.263023    8.986424\n",
      " 14.467513    7.018659   11.013431   14.887043   14.954937   12.972555\n",
      "  9.014234    6.976498    1.1059196  15.931949   18.255627   11.972846\n",
      " 10.969894   14.009459    2.029747    6.9864316   6.9639206   8.965218\n",
      "  9.965648    6.9588075  12.025005    0.99458736 11.051502   12.0466585\n",
      " 12.680671    4.162377    5.9779253   9.025305   14.98016    11.031685\n",
      "  5.99851    18.00417     6.987452    9.9733715   7.986602    9.99633\n",
      "  9.012501   11.003187    6.983654    6.023259    9.989405    8.993708\n",
      " 12.026282    9.983224   11.007946    9.009978    9.979712    9.960366\n",
      "  9.02959     6.021975   15.0769415   4.1854873   6.016715    8.989953\n",
      "  3.9614236  12.082245   13.01227    13.001752   10.037073   13.050227\n",
      " 12.008015    4.9822674  12.771736    8.012871   14.042867    9.987578\n",
      "  6.019084   11.010932   10.970497   13.9512005   9.967795   12.957376\n",
      " 15.984414    6.0108304   6.026584   18.2012      8.023145   12.985558\n",
      "  9.02737    14.926674    7.99999    11.011733   14.019522    8.000067\n",
      "  8.9966135   7.990679   12.007255    7.918762    6.9811754   8.002869\n",
      "  5.9793715   6.009261    6.9876165   6.978268    8.00356     3.9311304\n",
      " 12.0144615   4.991911    9.937679    8.014673    9.023552    6.020154\n",
      "  9.970172    6.0007544  12.882933   12.035282    7.9983916   5.013868\n",
      " 12.971178    9.96786     6.986193    8.024163   13.005898    6.9804134\n",
      "  6.007822   15.217907   17.123987   11.964839   11.083982   16.035175\n",
      "  9.986258   14.8720455  12.024676   19.415205    9.043212    7.9960327\n",
      " 10.024638    6.983141    3.9413834  15.825993    6.0077453  16.39977\n",
      "  6.000593    4.9862614   9.061738   11.023997   12.982243    6.0004287\n",
      "  7.0386395   9.972464    7.9885497  11.017686    9.0041065   6.9795766\n",
      " 15.642674    9.967848   13.95476     6.9912825   6.0210104   4.9882846\n",
      " 12.955084    8.000665   12.04802     8.032854    8.004023   11.997859\n",
      "  5.9803076   9.95365    17.272423    9.00483     9.0008955   9.012157\n",
      " 15.261717   12.983099    8.010246   14.998827    6.9736633   9.01158\n",
      " 12.955739   10.968864   11.008804    6.984553    9.024043   11.996352\n",
      " 11.991304    9.96799   ]\n"
     ]
    }
   ],
   "source": [
    "# Extract the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Initialize the XGBRegressor with the best parameters\n",
    "best_xgb = XGBRegressor(**best_params)\n",
    "\n",
    "# Fit the model to the full training data\n",
    "best_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set or future data\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "# Display predictions\n",
    "print(\"Predictions:\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 0.27854613112538207\n",
      "Mean Absolute Error (MAE): 0.07478642561574922\n",
      "R-squared (R²): 0.9934741258621216\n"
     ]
    }
   ],
   "source": [
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Display the metrics\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-squared (R²):\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb_urgent.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(best_xgb, 'xgb_urgent.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
